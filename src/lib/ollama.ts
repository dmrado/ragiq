import {execSync} from "child_process";
import {Ollama} from "ollama";

const ollamaClient = new Ollama({host: "http://localhost:11434"});
const EMBEDDING_MODEL = "nomic-embed-text";

/**
 * –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∞–º—É—é –ª—ë–≥–∫—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ —Å–ø–∏—Å–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö.
 * –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ‚Äî –ø—Ä–æ–±—É–µ—Ç pull.
 */
export async function getAvailableLLM(): Promise<string> {
    try {
        const result = execSync("ollama list", {encoding: "utf-8"});
        const lines = result.split("\n").slice(1);
        const models = lines
            .map((line) => line.trim().split(/\s+/)[0])
            .filter(Boolean);

        // –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ª—ë–≥–∫–∏—Ö –º–æ–¥–µ–ª–µ–π todo —Å–¥–µ–ª–∞—Ç—å –≤—ã–±–æ—Ä –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ—â–Ω–æ—Å—Ç–µ–π
        const preferred = [
            "llama3.1:1b", // ~1.0B
            "deepseek-coder:1.3b-base", // ~1.3B
            "llama3.1:3b-instruct-q4_K_M", // ~3.0B
            "phi3:mini", // ~3.8B
            "deepseek-coder:6.7b-instruct", // ~6.7B
            "mistral:7b-instruct-q4_K_M", // ~7.0B
            "llama3.1:8b-instruct-q4_K_M", // ~8.0B
            "llama3:latest",
        ];

        for (const p of preferred) {
            if (models.includes(p)) {
                console.log(`‚úÖ –í—ã–±—Ä–∞–Ω–∞ —Å–∞–º–∞—è –ª—ë–≥–∫–∞—è –¥–æ—Å—Ç—É–ø–Ω–∞—è –º–æ–¥–µ–ª—å: ${p}`);
                return p;
            }
        }

        // fallback
        console.warn("‚ö†Ô∏è –ü–æ–¥—Ö–æ–¥—è—â–∞—è –ª—ë–≥–∫–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é...");
        for (const p of preferred) {
            try {
                console.log(`‚¨áÔ∏è –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å ${p}...`);
                await ollamaClient.pull({model: p});
                console.log(`‚úÖ –ú–æ–¥–µ–ª—å ${p} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≤—ã–±—Ä–∞–Ω–∞.`);
                return p;
            } catch (err: any) {
                console.warn(`‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å ${p}: ${err.message}`);
            }
        }

        throw new Error("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama.");
    } catch (err: any) {
        console.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π:", err.message);
        return "llama3.1:3b-instruct-q4_K_M"; // –±–µ–∑–æ–ø–∞—Å–Ω—ã–π fallback
    }
}

/**
 * –û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –≤ Ollama LLM
 */
export async function askOllama(prompt: string): Promise<string> {
    const model = await getAvailableLLM();
    console.log(`üß† –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM: ${model}`);

    try {
        const response = await ollamaClient.chat({
            model,
            messages: [{role: "user", content: prompt}],
        });
        return response.message?.content ?? "";
    } catch (err: any) {
        console.error(`‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å Ollama (${model}):`, err.message);
        return "‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ LLM.";
    }
}

/**
 * –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ Ollama
 */
export async function getEmbeddings(texts: string[]): Promise<number[][]> {
    const responses = await Promise.all(
        texts.map(async (text) => {
            const res = await ollamaClient.embeddings({
                model: EMBEDDING_MODEL,
                prompt: text,
            });
            return res.embedding;
        })
    );

    return responses;
}
